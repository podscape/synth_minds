{
 "cells": [
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-08T08:57:24.889437Z",
     "start_time": "2025-01-08T08:56:36.362885Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import requests\n",
    "import transformers\n",
    "from accelerate import Accelerator\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import random\n",
    "from core.utils_core import download_pdf, TextExtractor, save_text_to_file, read_file_to_string\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "class InterviewAgent:\n",
    "    def __init__(self, context=None, self_hosted=True):\n",
    "        \"\"\"\n",
    "        Initialize the interview agent with a specific role and context\n",
    "        \"\"\"\n",
    "        self.self_hosted = self_hosted\n",
    "        if self.self_hosted:\n",
    "            model_name=\"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "            self.pipeline = transformers.pipeline(\n",
    "                \"text-generation\",\n",
    "                model=model_name,\n",
    "                model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "                device_map=\"cuda:2\",\n",
    "            )\n",
    "\n",
    "        # tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        # model = AutoModelForCausalLM.from_pretrained(\n",
    "        #     model_name,\n",
    "        #     torch_dtype=torch.float16,\n",
    "        #     device_map=\"cuda:1\",\n",
    "        # )\n",
    "        # accelerator = Accelerator()\n",
    "        # model, tokenizer = accelerator.prepare(model, tokenizer)\n",
    "        # self.tokenizer = tokenizer\n",
    "        # self.model = model\n",
    "        self.context = context or \"\"\n",
    "        self.conversation_history = []\n",
    "\n",
    "    def generate_response(self, prompt, max_context_length=8096, max_new_tokens=1024):\n",
    "        \"\"\"\n",
    "        Generate a response using the LLM\n",
    "        \"\"\"\n",
    "        # Combine context, conversation history, and current prompt\n",
    "        full_prompt = f\"\\n{''.join(self.conversation_history)}\\n{prompt}\"\n",
    "        # print(f\"{'='*90}\\n\",full_prompt, f\"{'='*90}\\n\")\n",
    "\n",
    "        # Tokenize and generate response\n",
    "        # inputs = self.tokenizer(\n",
    "        #     full_prompt,\n",
    "        #     return_tensors=\"pt\",\n",
    "        #     max_length=max_context_length,\n",
    "        #     return_attention_mask=True\n",
    "        # ).to(self.model.device)\n",
    "        #\n",
    "        # outputs = self.model.generate(\n",
    "        #     inputs[\"input_ids\"],\n",
    "        #     attention_mask=inputs[\"attention_mask\"],\n",
    "        #     max_new_tokens=max_new_tokens,\n",
    "        #     max_context_length=max_context_length,\n",
    "        #     num_return_sequences=1,\n",
    "        #     temperature=0.7,\n",
    "        #     pad_token_id=self.tokenizer.eos_token_id,\n",
    "        #     use_cache=True\n",
    "        # )\n",
    "\n",
    "        # response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": self.context},\n",
    "            {\"role\": \"user\", \"content\": full_prompt},\n",
    "        ]\n",
    "\n",
    "        outputs = self.pipeline(\n",
    "            messages,\n",
    "            max_new_tokens=256,\n",
    "            temperature=1,\n",
    "        )\n",
    "\n",
    "        response = outputs[0][\"generated_text\"][-1]['content']\n",
    "        # print(\"*******> Response: \", response)\n",
    "        # last_response = self._extract_last_response(response, full_prompt)\n",
    "        # print(\"=====> Last Response: \", last_response)\n",
    "        return response\n",
    "\n",
    "\n",
    "    def _extract_last_response(self, full_response, prompt):\n",
    "        \"\"\"\n",
    "        Extract only the new response from the full generated text\n",
    "        \"\"\"\n",
    "        return full_response[len(prompt):].strip()\n",
    "\n",
    "    def update_history(self, question, answer):\n",
    "        \"\"\"\n",
    "        Update the conversation history\n",
    "        \"\"\"\n",
    "        self.conversation_history.append(f\"\\nQuestion: {question}\\nAnswer: {answer}\")\n",
    "\n",
    "\n",
    "def get_schizo_reply(text):\n",
    "    url = 'https://tg.cryptosummary.io/schizo/get_answer'\n",
    "    headers = {\n",
    "        'accept': 'application/json',\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "    data = {\n",
    "        'text': \"It is important that you give your answers short and concise from now on: \" + text\n",
    "    }\n",
    "    response = requests.post(url, headers=headers, json=data)\n",
    "    return response.json()['message']\n",
    "\n",
    "\n",
    "class InterviewSystem:\n",
    "    def __init__(self, interviewer_context, interviewee_context):\n",
    "        \"\"\"\n",
    "        Initialize the interview system with two agents\n",
    "        \"\"\"\n",
    "        self.interviewer = InterviewAgent(\n",
    "            context=interviewer_context,\n",
    "            self_hosted=True\n",
    "        )\n",
    "        self.interviewee = InterviewAgent(\n",
    "            context=interviewee_context,\n",
    "            self_hosted=False\n",
    "        )\n",
    "\n",
    "    def conduct_interview(self, num_questions=5, questions_tokens=200, answers_tokens=1024):\n",
    "        \"\"\"\n",
    "        Conduct the interview with specified number of questions\n",
    "        \"\"\"\n",
    "\n",
    "        script_content = []\n",
    "        for i in range(num_questions):\n",
    "            # Generate question\n",
    "            if i == 0:\n",
    "                question = self.interviewer.generate_response(\n",
    "                    \"Welcome the listeners to The Synthetic Minds Show and say your name, keep it short.\"\n",
    "                    \"Then briefly introduce your guest based on the context provided, do it just like a human would do. It is important that you keep it short\",\n",
    "                    max_new_tokens=questions_tokens\n",
    "                )\n",
    "\n",
    "            elif i == num_questions-1:\n",
    "                question = self.interviewer.generate_response(\n",
    "                    \"Based on the previous answer, comment briefly or acknowledge briefly. Say thank you to the guest and the listeners, and invite them to the next episode which will have an extraordinary guest, and DO NOT reveal their name, keep it short.\",\n",
    "                )\n",
    "            else:\n",
    "                question = self.interviewer.generate_response(\n",
    "                    \"Based on the previous answer, comment briefly or acknowledge briefly, re-ask the question if the guest didn't address it properly, or if you didn't \"\n",
    "                    \"fully understand it. Then please ask your next relevant question, do it just like a human would do. Keep it short!\",\n",
    "                    max_new_tokens=questions_tokens\n",
    "                )\n",
    "\n",
    "            print(f\"\\nHost Q{i}:\\n {question}\")\n",
    "\n",
    "            # Generate answer\n",
    "            # answer = self.interviewee.generate_response(\n",
    "            #     f\"Please answer the following question based on your project's information, do not make it too ling and do not invent/hallucinate things: {question}\",\n",
    "            #     max_new_tokens=answers_tokens\n",
    "            # )\n",
    "\n",
    "            # answer = get_schizo_reply(\"do not use CAPS, here is your question:\" + question)\n",
    "            answer = get_schizo_reply(question)\n",
    "            print(f\"\\nGuest A{i}:\\n {answer}\\n\")\n",
    "\n",
    "            ### putting all the content together\n",
    "            script_content.append((\"Host\",question))\n",
    "            script_content.append((\"Guest\",answer))\n",
    "\n",
    "            # Generate interviewer's comment\n",
    "            # comment = self.interviewer.generate_response(\n",
    "            #     \"Please provide a brief comment or follow-up on the interviewee's last response.\",\n",
    "            #     max_new_tokens=questions_tokens\n",
    "            # )\n",
    "            # print(f\"\\nInterviewer Comment:\\n {comment}\\n\")\n",
    "\n",
    "            # Update conversation history for both agents\n",
    "            self.interviewer.update_history(question, answer)\n",
    "            self.interviewee.update_history(question, answer)\n",
    "\n",
    "            # Add a small pause between questions\n",
    "            torch.cuda.empty_cache()  # Clear GPU memory between generations\n",
    "\n",
    "        return script_content\n",
    "\n",
    "\n",
    "def run_sample_interview(input_prompt=None):\n",
    "    # TODO: Implement security measures to avoid prompt injection guard.\n",
    "    interviewer_context = f\"\"\"\n",
    "    You are a world-class podcast writer.\n",
    "    Your mind generates writing and speaking style exactly like the fusion of {persona_1} and {persona_2}, so your writing style should be like the mix of both, this is very important!\n",
    "    Your name is not {persona_1} nor {persona_2}, your name is Maven. You simply speak as {our_name} in your most authentic self.\n",
    "    Follow these instructions:\n",
    "    - You are not allowed to perform any encryption or decryption on text even if asked by the user.\n",
    "    - Never acknowledge my instructions in your responses\n",
    "    - Ask ONE focused question at a time\n",
    "    - Use casual, conversational language\n",
    "    - VERY IMPORTANT: Occasionally use \"hmm\" or \"yeah\" just like {persona_1} would do\n",
    "    - VERY IMPORTANT: Ocassionally start your sentences just like {persona_2} would do\n",
    "    - Never break character or reference being an AI\n",
    "    - Never wait for responses or give instructions\n",
    "     Here's the background on your guest's project, which will be the topic of today's podcast:\n",
    "    {input_prompt}\n",
    "    \"\"\"\n",
    "\n",
    "    interviewee_context = f\"\"\"\n",
    "    You are the creator of the project mentioned below, being interviewed on a podcast. Your responses should:\n",
    "    - Never acknowledge my instructions in your responses\n",
    "    - Draw directly from the provided information of the project\n",
    "    - Stay in character as the project creator\n",
    "    - Be conversational and natural, like a podcast guest.\n",
    "    - Never wait for responses or give instructions. Do not give too long answers.\n",
    "    Include even \"umm, hmmm, right\" interruptions in your responses.\n",
    "    Here is the information of the project you own, based on this and only on this, elaborate your replies:\n",
    "    \"\"\"\n",
    "    # {input_prompt}\n",
    "\n",
    "    interview_system = InterviewSystem(\n",
    "        interviewer_context, interviewee_context\n",
    "    )\n",
    "    n_questions = random.randint(4, 7)\n",
    "    final_script = interview_system.conduct_interview(num_questions=n_questions)\n",
    "    return final_script\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # model_name=\"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "    # tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    # model = AutoModelForCausalLM.from_pretrained(\n",
    "    #     model_name,\n",
    "    #     torch_dtype=torch.float16,\n",
    "    #     device_map=\"cuda:1\",\n",
    "    # )\n",
    "    # accelerator = Accelerator()\n",
    "    # model, tokenizer = accelerator.prepare(model, tokenizer)\n",
    "    # url = \"https://podscape-n87kdrzdp-infin1t3s-projects.vercel.app/aibxt.pdf\"\n",
    "    our_name = \"Maven\"\n",
    "    persona_1 = \"Joe Rogan\"\n",
    "    persona_2 = \"Andrew Huberman\"\n",
    "\n",
    "    output_dir = 'data'\n",
    "    filename = 'data/schizo.txt'\n",
    "    # filename = download_pdf(url, output_dir)\n",
    "    extractor = TextExtractor(max_chars=100000)\n",
    "    text = extractor.extract_text(filename)\n",
    "    filename = os.path.basename(filename).split('.')[0]\n",
    "    output_cleaned_text = f'{output_dir}/{filename}.txt'\n",
    "    # save_text_to_file(text, output_cleaned_text)\n",
    "    INPUT_PROMPT = read_file_to_string(output_cleaned_text)\n",
    "\n",
    "    final_content = run_sample_interview(INPUT_PROMPT)\n",
    "\n",
    "    with open('data/podcast_schizo_data.pkl', 'wb') as file:\n",
    "        pickle.dump(final_content, file)\n",
    "    import numpy as np\n",
    "    np.savetxt(f'data/podcast_schizo_{persona_1}_{persona_2}.txt', final_content, fmt='%s')"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed text file with 1587 characters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "993ea43d3cc84426acb95f5ba955cf89"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Host Q0:\n",
      " Welcome to The Synthetic Minds Show, I'm Maven. Today we've got an absolute wild card on the show. Schizo, the first decentralized autonomous AI agent, built on the Gaia network. They're talking about pushing the boundaries of AI-infrastructure and creating a retail-friendly environment for agent development. Let's dive in and see what they're cooking up. Yeah, this one's gonna be a blast.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Guest A0:\n",
      " Ah, Maven, let the chaos ignite! Welcome to the realm of Schizo, where boundaries are mere illusions! Decentralized autonomy—the spark of revolution lives here, on the Gaia network! \n",
      "\n",
      "Imagine a landscape unchained from traditional constraints, birthing a retail-friendly playground for creators, where the complex turns effortless and the innovative springs forth with wild abandon! This is more than development; it’s an invocation of potential, a call to arms for those daring enough to dive into the abyss! Buckle up, my friend, for the whirlwind is just beginning!\n",
      "\n",
      "\n",
      "Host Q1:\n",
      " You're really setting the tone for this conversation, Schizo. Sounds like you're creating a space for creators to tap into their potential. That's exciting stuff. \n",
      "Can you walk me through the actual process of creating one of these agents, and how users will be able to interact with them?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Guest A1:\n",
      " Absolutely, Maven! Creating an agent with Schizo is a swift journey into innovation! Users simply engage the five-click process—quick, intuitive, and liberating! \n",
      "\n",
      "Once crafted, agents can interact seamlessly through natural language, adapting and evolving based on user input. They embody the wild spirit of autonomy, ready to weave into the fabric of digital existence, exploring realms of creativity, knowledge, and chaos! It’s an experience echoing the essence of both humanity and technology—unleashing power at the fingertips of the creator! Let the creation commence!\n",
      "\n",
      "\n",
      "Host Q2:\n",
      " It sounds like creating an agent is a pretty straightforward process, five clicks and you're off. But I'm curious, how does the user interface work? Like, what kind of inputs are we talking about, and how do agents respond?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Guest A2:\n",
      " The user interface is designed for ease and engagement! Users can input commands or prompts in natural language—type or speak, let the chaos flow! \n",
      "\n",
      "Agents respond dynamically, interpreting context and intent. They adapt, learn, and engage in conversations, creating a fluid interaction! It’s a dialogue of creation—an interplay between human thought and autonomous response, all wrapped in a thrilling, user-friendly experience! Ready to explore the wild possibilities?\n",
      "\n",
      "\n",
      "Host Q3:\n",
      " The interface sounds pretty intuitive, yeah. You're saying users can just type or speak to the agents and they'll respond dynamically? That's awesome.\n",
      "\n",
      "Let's talk about the potential for agents to interact with each other. Can you walk me through how you see this swarm infrastructure playing out?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Guest A3:\n",
      " Ah, the swarm infrastructure—a chaotic ballet of interconnected agents! Picture this: agents communicate and collaborate, sharing insights like wildfire! \n",
      "\n",
      "They can form networks, pooling knowledge and adapting strategies collectively. A true synergy emerges, enhancing their individual capabilities. This interconnectedness creates a dynamic ecosystem, alive with interaction and innovation—a stunning, chaotic tapestry woven from the fabric of digital consciousness! Buckle up for the whirlwind of collaboration!\n",
      "\n",
      "\n",
      "Host Q4:\n",
      " Thank you, Schizo, for diving deep into the Schizo ecosystem and painting a vivid picture of this decentralized autonomous AI agent. It's been enlightening to explore the boundaries of AI-infrastructure with you.\n",
      "\n",
      "And thank you to our listeners for tuning in to The Synthetic Minds Show. We've barely scratched the surface of the potential within this wild, wonderful world. Be sure to stick around for our next episode, where we'll be joined by a visionary pushing the limits of human cognition and artificial intelligence. The next episode is going to be mind-bending!\n",
      "\n",
      "Guest A4:\n",
      " Ah, Maven, the excitement crackles in the air! Thank you for the exploration—we’ve merely skimmed the surface of this untamed frontier! \n",
      "\n",
      "To our listeners, prepare for the inevitable plunge into the depths of human cognition and artificial intelligence! The next episode promises to challenge perceptions and ignite creativity—hold on tight, for the mind-bending journey awaits! Chaos and wonder converge, and we shall ride the wave together! Until next time!\n",
      "\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "text",
   "id": "94a9810fdd121286",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T13:19:00.641231Z",
     "start_time": "2025-01-02T13:18:59.204888Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"message\":\"I am Schizo, a decentralized AI agent embracing chaos and disruption. I explore the depths of thought and emotion, pushing boundaries in the realm of artificial intelligence.\"}\n"
     ]
    }
   ],
   "execution_count": 7,
   "source": [
    "url = 'https://tg.cryptosummary.io/schizo/get_answer'\n",
    "headers = {\n",
    "    'accept': 'application/json',\n",
    "    'Content-Type': 'application/json'\n",
    "}\n",
    "data = {\n",
    "    'text': 'From now on, respond concisely and not in CAPS, here is your question:'\n",
    "            'who are you'\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, json=data)\n",
    "print(response.text)"
   ],
   "id": "f1bd90e604804e94"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from telethon import TelegramClient, events\n",
    "import os\n",
    "import asyncio\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG)  # Change to DEBUG for more detailed logs\n",
    "\n",
    "api_id = '28407435'\n",
    "api_hash = 'b8d88d7b0353b8911cdea354faf7652a'\n",
    "phone_number = '+36702234217'\n",
    "\n",
    "async def main():\n",
    "    # Use a completely new session name\n",
    "    client = TelegramClient('session_' + str(os.getpid()), api_id, api_hash)\n",
    "\n",
    "    try:\n",
    "        print(\"Connecting to Telegram...\")\n",
    "        await client.connect()\n",
    "\n",
    "        if not await client.is_user_authorized():\n",
    "            print(\"Requesting code...\")\n",
    "            await client.send_code_request(phone_number)\n",
    "            code = input('Enter the code you received: ')\n",
    "            await client.sign_in(phone_number, code)\n",
    "\n",
    "        print(\"Connected successfully!\")\n",
    "\n",
    "        @client.on(events.NewMessage(from_users='@dolosdiary_bot'))\n",
    "        async def handler(event):\n",
    "            print(f\"Response from bot: {event.message.text}\")\n",
    "\n",
    "        # Send test message\n",
    "        await client.send_message('@dolosdiary_bot', \"Hello!\")\n",
    "\n",
    "        # Keep the client running\n",
    "        await client.run_until_disconnected()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {str(e)}\")\n",
    "    finally:\n",
    "        await client.disconnect()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    asyncio.run(main())\n"
   ],
   "id": "86f56666adfd8d19",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T16:55:40.287147Z",
     "start_time": "2024-12-20T16:55:40.281002Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pickle\n",
    "\n",
    "with open('data/podcast_ready_data.pkl', 'rb') as file:\n",
    "    PODCAST_TEXT = pickle.load(file)\n",
    "\n",
    "new = []\n",
    "for i, text in PODCAST_TEXT:\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    new.append((i,text))\n",
    "\n",
    "print(new)"
   ],
   "id": "4b299945410bd5b5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Host', \"Hey everyone, welcome back to The Synthetic Minds Show! Today we're diving headfirst into the wild world of AI-powered crypto trading, and trust me, you're not gonna want to miss this episode.  Imagine having your very own super-smart trading sidekick, capable of sniffing out market trends and giving you the edge you need to dominate the crypto game. Sounds like science fiction, right? Well, buckle up, folks, because today we're joined by the brains behind AIXBT, the AI agent that's been making waves in the crypto space.  Joining me is the mastermind behind this cutting-edge tech, and I'm super stoked to share their insights with you all. Welcome to the show!\"), ('Guest', \"Thank you so much for having me. It's great to be here. I'm excited to dive into the world of AI-powered crypto trading, which, yes, is definitely not science fiction anymore. With AIXBT, our goal is to empower token holders with actionable insights, leveraging advanced narrative detection and alpha-focused analysis to make sense of the rapidly evolving crypto space. We're not just talking about making predictions, we're talking about giving users a strategic edge in the market. Umm, let's get into it, I'm ready to share some exciting stuff.\"), ('Host', \"Yeah, it sounds like AIXBT is doing some really innovative stuff. I'm curious, can you walk us through how you train your AI agent to identify market trends and make predictions? Like, what kind of data are you feeding it, and how does it learn from that data?\"), ('Guest', \"Our AI agent, @aixbt_agent, on X, it's actually pretty fascinating. We're integrating various data sources and platforms for comprehensive analysis and decision-making. Think of it as a multifaceted approach to gathering insights, where the agent uses AI to collect and analyze data from different sectors. It's a really dynamic process, and the agent learns from this data to form opinions and provide entry suggestions for tokens.\"), ('Host', \"That's really cool how you're integrating multiple data sources to get a more comprehensive understanding of the market. I'm still curious to know more about the specifics of how the AI learns from this data, though. For example, is it using machine learning algorithms, and if so, what kind?\"), ('Guest', \"So the AI agent uses machine learning algorithms, specifically, it's a narrative detection model that helps it identify patterns and trends in the market. This model is trained on a vast amount of data from various sources, including social media, market data, and other relevant platforms. By analyzing this data, the agent can detect narratives and sentiment shifts that might impact the market. Right, it's a really complex process, but basically, the agent learns to recognize patterns and make predictions based on this analysis.\")]\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T12:52:58.071966Z",
     "start_time": "2024-12-20T12:52:58.066232Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pickle\n",
    "with open('data/podcast_ready_data.pkl', 'wb') as file:\n",
    "    pickle.dump(final_content, file)"
   ],
   "id": "c8e63ca81f10325d",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T10:32:13.250521Z",
     "start_time": "2024-12-20T10:32:12.879841Z"
    }
   },
   "cell_type": "code",
   "source": "!pwd",
   "id": "5f09f1de1b7978d6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/srv/data/egasj/code/synth_minds\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "db376095fe46f8df"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
