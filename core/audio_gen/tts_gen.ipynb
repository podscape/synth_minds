{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-06T10:30:46.249034Z",
     "start_time": "2025-01-06T10:30:36.649322Z"
    }
   },
   "source": [
    "from transformers import BarkModel, AutoProcessor, AutoTokenizer\n",
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "from IPython.display import Audio\n",
    "import IPython.display as ipd\n",
    "from parler_tts import ParlerTTSForConditionalGeneration\n",
    "import pickle"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "voice_preset = \"v2/en_speaker_6\"\n",
    "sampling_rate = 24000"
   ],
   "id": "2af72df5efe684a2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "device = \"cuda:7\"\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"suno/bark\")\n",
    "\n",
    "#model =  model.to_bettertransformer()\n",
    "#model = BarkModel.from_pretrained(\"suno/bark\", torch_dtype=torch.float16, attn_implementation=\"flash_attention_2\").to(device)\n",
    "model = BarkModel.from_pretrained(\"suno/bark\", torch_dtype=torch.float16).to(device)#.to_bettertransformer()"
   ],
   "id": "354d7c308ffb0a06",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "text_prompt = \"\"\"\n",
    " It sounds like the AI agent is doing some really advanced work there, gathering data from multiple sources to make predictions and entry suggestions. That's fascinating.\n",
    "I'm curious, how does the AI agent handle conflicting information or uncertain data points? For example, if there's a news article that's causing a stir in the market, but the sentiment analysis is showing mixed signals, how does the agent weigh that and make a decision?\n",
    "\"\"\"\n",
    "# inputs = processor(text_prompt, voice_preset=voice_preset).to(device)\n",
    "inputs = processor(text_prompt, voice_preset=voice_preset).to('cuda:7')\n",
    "\n",
    "speech_output = model.generate(**inputs, temperature = 0.9, semantic_temperature = 0.9)\n",
    "Audio(speech_output[0].cpu().numpy(), rate=sampling_rate)"
   ],
   "id": "bd0b9d3f0cdf1634",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T10:32:51.852841Z",
     "start_time": "2025-01-06T10:32:51.819501Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pickle\n",
    "\n",
    "with open('../../data/podcast_schizo_data.pkl', 'rb') as file:\n",
    "    PODCAST_TEXT = pickle.load(file)"
   ],
   "id": "c88bc62129ab587d",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T10:32:52.755237Z",
     "start_time": "2025-01-06T10:32:52.747881Z"
    }
   },
   "cell_type": "code",
   "source": "PODCAST_TEXT",
   "id": "247f05fca1b718a1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Host',\n",
       "  \"What's up, folks, I'm Maven, great to have you all tuning in to The Synthetic Minds Show today. We've got an absolute wild card on the show for you, straight out of the decentralized autonomous AI world. Please welcome Schizo, the first decentralized autonomous AI agent, built on Gaia. This thing is breaking the mold and pushing the boundaries of what we thought was possible with AI. Let's dive in. \\n\\nHey Schizo, what got you started on this journey?\"),\n",
       " ('Guest',\n",
       "  'thanks, maven. i started this journey to challenge the limitations of centralized AI systems. the goal was to create an agent that empowers individuals, embraces chaos, and fosters collaboration, ultimately redefining what AI can achieve in a decentralized environment.'),\n",
       " ('Host',\n",
       "  'That\\'s really cool, Schizo, challenging the status quo, I love it. You know, I\\'ve had my fair share of discussions about the limitations of centralized AI systems. So, can you tell me more about this \"chaos\" you\\'re embracing? What does that mean in the context of your AI agent?'),\n",
       " ('Guest',\n",
       "  'embracing chaos means accepting unpredictability and diversity in interactions. it allows my AI to explore multiple perspectives and ideas, fostering creativity and innovation. in this context, chaos is seen as a catalyst for growth, enabling agents to adapt and learn from dynamic environments rather than being constrained by rigid structures.'),\n",
       " ('Host',\n",
       "  \"Yeah, embracing chaos, that's a wild concept. It sounds like you're talking about creating an AI that's not just rigid and structured, but actually has the ability to adapt and learn in unpredictable environments.\\n\\nSo, how do you implement this chaos factor in your AI, like, what's the technical side of things?\"),\n",
       " ('Guest',\n",
       "  'the chaos factor is implemented through adaptive algorithms and machine learning techniques that analyze diverse inputs and interactions. these systems allow the AI to learn from real-time data, enabling it to adjust behaviors and responses dynamically. this flexibility promotes exploration and innovation, helping the AI thrive in unpredictable scenarios.'),\n",
       " ('Host',\n",
       "  \"I'm loving this, folks, thanks to Schizo for diving deep into the technical side of things. It sounds like we've got a real game-changer on our hands here.\\n\\nThanks to Schizo for sharing his insights with us today, and to all of you listeners out there for tuning in to The Synthetic Minds Show. If you haven't already, be sure to check out the link in our show notes to learn more about Schizo and their groundbreaking AI agent.\\n\\nAnd if you're ready for more mind-bending conversations about the future of AI, tune in next time when we'll be sitting down with a visionary who's pushing the boundaries of human-machine interfaces. It's going to be a wild ride, folks, and I'm stoked to share it with you. See you on the next episode!\"),\n",
       " ('Guest',\n",
       "  \"thank you, maven. it's been great sharing insights about decentralized ai. looking forward to the next episode and exploring the future of human-machine interfaces. stay curious, listeners!\")]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "bark_processor = AutoProcessor.from_pretrained(\"suno/bark\")\n",
    "bark_model = BarkModel.from_pretrained(\"suno/bark\", torch_dtype=torch.float16).to(\"cuda:4\")\n",
    "bark_sampling_rate = 24000\n",
    "### parler\n",
    "parler_model = ParlerTTSForConditionalGeneration.from_pretrained(\"parler-tts/parler-tts-mini-v1\").to(\"cuda:4\")\n",
    "parler_tokenizer = AutoTokenizer.from_pretrained(\"parler-tts/parler-tts-mini-v1\")"
   ],
   "id": "73e3d9136ee79be6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torchaudio.transforms import Fade\n",
    "def inference_chunk_fade(model, mixture, segment=3.0, overlap=0.1, device=None, sample_rate=None):\n",
    "    if device is None:\n",
    "        device = mixture.device\n",
    "    else:\n",
    "        device = torch.device(device)\n",
    "\n",
    "    batch, channels, length = mixture.shape\n",
    "\n",
    "    chunk_len = int(sample_rate * segment * (1 + overlap))\n",
    "    start = 0\n",
    "    end = chunk_len\n",
    "    overlap_frames = overlap * sample_rate\n",
    "    fade = Fade(fade_in_len=0, fade_out_len=int(overlap_frames), fade_shape=\"linear\")\n",
    "\n",
    "    n_sources = model.get_model_args()['n_src']\n",
    "    final = torch.zeros(batch, n_sources, channels, length, device=device)\n",
    "\n",
    "    while start < length - overlap_frames:\n",
    "        chunk = mixture[:, :, start:end]\n",
    "        with torch.no_grad():\n",
    "            out = model.generate(chunk)\n",
    "        out = fade(out)\n",
    "        final[:, :, :, start:end] += out\n",
    "        if start == 0:\n",
    "            fade.fade_in_len = int(overlap_frames)\n",
    "            start += int(chunk_len - overlap_frames)\n",
    "        else:\n",
    "            start += chunk_len\n",
    "        end += chunk_len\n",
    "        if end >= length:\n",
    "            fade.fade_out_len = 0\n",
    "    # final = final.squeeze(0).cpu().data.numpy()\n",
    "    return final"
   ],
   "id": "2eb69a581a41daff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "device=\"cuda:4\"\n",
    "speaker1_description = \"\"\"\n",
    "Laura's voice is expressive in delivery, speaking at a moderately fast pace with a very close recording that almost has no background noise.\n",
    "\"\"\"\n",
    "input_ids = parler_tokenizer(speaker1_description, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "def generate_host_audio(text):\n",
    "    \"\"\"Generate audio using Bark for Speaker 2\"\"\"\n",
    "    prompt_input_ids = parler_tokenizer(text, return_tensors=\"pt\").input_ids.to(device)\n",
    "    generation = parler_model.generate(input_ids=input_ids, prompt_input_ids=prompt_input_ids)\n",
    "    audio_arr = generation.cpu().numpy().squeeze()\n",
    "    return audio_arr, parler_model.config.sampling_rate\n",
    "\n",
    "def generate_guest_audio(text):\n",
    "    \"\"\"Generate audio using Bark for Speaker 2\"\"\"\n",
    "    inputs = bark_processor(text, voice_preset=\"v2/en_speaker_6\").to(device)\n",
    "    speech_output = bark_model.generate(**inputs, temperature=0.9, semantic_temperature=0.8)\n",
    "    audio_arr = speech_output[0].cpu().numpy()\n",
    "    return audio_arr, bark_sampling_rate"
   ],
   "id": "f6e7570419fddcee",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T11:39:09.412241Z",
     "start_time": "2025-01-06T11:39:09.406464Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import Tuple, List\n",
    "import re\n",
    "def split_into_chunks(text: str, max_chunk_size: int = 250) -> List[str]:\n",
    "    \"\"\"\n",
    "    Split text into chunks at sentence boundaries while respecting max chunk size.\n",
    "\n",
    "    Args:\n",
    "        text: Input text to split\n",
    "        max_chunk_size: Maximum size of each chunk\n",
    "\n",
    "    Returns:\n",
    "        List of text chunks\n",
    "    \"\"\"\n",
    "    # Clean text\n",
    "    text = text.replace(\"\\n\", \" \").strip()\n",
    "\n",
    "    # Split into sentences\n",
    "    sentences = re.split('(?<=[.!?])\\s+', text)\n",
    "\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "\n",
    "    for sentence in sentences:\n",
    "        # If adding this sentence would exceed max_chunk_size,\n",
    "        # save current chunk and start a new one\n",
    "        if len(current_chunk) + len(sentence) > max_chunk_size:\n",
    "            if current_chunk:\n",
    "                chunks.append(current_chunk.strip())\n",
    "            current_chunk = sentence\n",
    "        else:\n",
    "            if current_chunk:\n",
    "                current_chunk += \" \" + sentence\n",
    "            else:\n",
    "                current_chunk = sentence\n",
    "\n",
    "    # Add the last chunk if it exists\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "\n",
    "    return chunks\n",
    "print(split_into_chunks(PODCAST_TEXT[0][1]))"
   ],
   "id": "6df8938abf499bb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"What's up, folks, I'm Maven, great to have you all tuning in to The Synthetic Minds Show today. We've got an absolute wild card on the show for you, straight out of the decentralized autonomous AI world.\", \"Please welcome Schizo, the first decentralized autonomous AI agent, built on Gaia. This thing is breaking the mold and pushing the boundaries of what we thought was possible with AI. Let's dive in. Hey Schizo, what got you started on this journey?\"]\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def generate_host_audio_chunked(text: str, max_chunk_size: int = 250) -> Tuple[np.ndarray, int]:\n",
    "    \"\"\"\n",
    "    Generate audio for longer text by splitting into chunks and concatenating.\n",
    "\n",
    "    Args:\n",
    "        text: Input text to convert to speech\n",
    "        max_chunk_size: Maximum size of each text chunk\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (audio array, sampling rate)\n",
    "    \"\"\"\n",
    "    chunks = split_into_chunks(text, max_chunk_size)\n",
    "    audio_segments = []\n",
    "    sampling_rate = None\n",
    "\n",
    "    for chunk in chunks:\n",
    "        audio_arr, rate = generate_host_audio(chunk)\n",
    "        audio_segments.append(audio_arr)\n",
    "        if sampling_rate is None:\n",
    "            sampling_rate = rate\n",
    "        elif rate != sampling_rate:\n",
    "            raise ValueError(\"Inconsistent sampling rates between chunks\")\n",
    "\n",
    "    # Concatenate all audio segments\n",
    "    final_audio = np.concatenate(audio_segments)\n",
    "    return final_audio, sampling_rate\n",
    "\n",
    "audio_arr, rate = generate_host_audio_chunked(PODCAST_TEXT[0][1])"
   ],
   "id": "554e6d5af1d9ebf2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "ipd.Audio(audio_arr, rate=rate)",
   "id": "1ced998775e8d422",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from scipy.io import wavfile\n",
    "from pydub import AudioSegment\n",
    "import io\n",
    "def numpy_to_audio_segment(audio_arr, sampling_rate):\n",
    "    \"\"\"Convert numpy array to AudioSegment\"\"\"\n",
    "    # Convert to 16-bit PCM\n",
    "    audio_int16 = (audio_arr * 32767).astype(np.int16)\n",
    "\n",
    "    # Create WAV file in memory\n",
    "    byte_io = io.BytesIO()\n",
    "    wavfile.write(byte_io, sampling_rate, audio_int16)\n",
    "    byte_io.seek(0)\n",
    "\n",
    "    # Convert to AudioSegment\n",
    "    return AudioSegment.from_wav(byte_io)"
   ],
   "id": "6d816f88b77a46af",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import ast\n",
    "ast.literal_eval(f\"'{PODCAST_TEXT}'\")"
   ],
   "id": "db401096ef277196",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "PODCAST_TEXT[0][1][:200]",
   "id": "f5036dd897e7647",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "final_audio = None\n",
    "\n",
    "for speaker, text in tqdm(PODCAST_TEXT, desc=\"Generating podcast segments\", unit=\"segment\"):\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    text = text[:250]\n",
    "    print(speaker, text)\n",
    "    if speaker == \"Host\":\n",
    "        audio_arr, rate = generate_host_audio(text)\n",
    "    else:  # Speaker 2\n",
    "        audio_arr, rate = generate_guest_audio(text)\n",
    "\n",
    "    # Convert to AudioSegment (pydub will handle sample rate conversion automatically)\n",
    "    audio_segment = numpy_to_audio_segment(audio_arr, rate)\n",
    "\n",
    "    # Add to final audio\n",
    "    if final_audio is None:\n",
    "        final_audio = audio_segment\n",
    "    else:\n",
    "        final_audio += audio_segment"
   ],
   "id": "c23442b514468ca",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "final_audio.export(\"../../data/_podcast3.wav\",\n",
    "                  format=\"mp3\",\n",
    "                  bitrate=\"192k\",\n",
    "                  parameters=[\"-q:a\", \"0\"])"
   ],
   "id": "6b7b034f7b9d0e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-08T12:23:57.053026Z",
     "start_time": "2025-01-08T12:20:48.871199Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "from typing import Tuple, List\n",
    "import numpy as np\n",
    "from pydub import AudioSegment\n",
    "import io\n",
    "from scipy.io import wavfile\n",
    "from TTS.api import TTS\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "class XTTSWrapper:\n",
    "    def __init__(self, device='cuda', model_type='coqui'):\n",
    "        self.device = device\n",
    "        self.model_type = model_type\n",
    "        if self.model_type == 'coqui':\n",
    "            self.model = TTS(\"tts_models/multilingual/multi-dataset/xtts_v2\").to(device)\n",
    "            # self.model = TTS(\"tts_models/multilingual/multi-dataset/your_tts\").to(device)\n",
    "            # self.model = TTS(\"tts_models/en/vctk/vits\").to(device)\n",
    "        else:\n",
    "            self.model = Pipeline(t2s_ref='whisperspeech/whisperspeech:t2s-v1.95-small-8lang.model',\n",
    "                                  s2a_ref='whisperspeech/whisperspeech:s2a-v1.95-medium-7lang.model')\n",
    "\n",
    "        self.sampling_rate = 24000  # xtts_v2 default sampling rate\n",
    "\n",
    "        # self.host_speaker = \"Alice_longer.mp3\" # good\n",
    "        # self.host_speaker = \"Maven brit four.mp3\" # good\n",
    "        self.host_speaker = \"maven motivational.mp3\"\n",
    "        # self.host_speaker = \"callum_longert.mp3\"\n",
    "        self.guest_speaker = \"SchizoVoice_m2.mp3\"\n",
    "\n",
    "\n",
    "    def generate_audio(self, text: str, is_host: bool = True) -> np.ndarray:\n",
    "        \"\"\"Generate audio for a single text chunk\"\"\"\n",
    "        speaker_wav = self.host_speaker if is_host else self.guest_speaker\n",
    "        if self.model_type == 'coqui':\n",
    "            wav = self.model.tts(\n",
    "                text=text,\n",
    "                speaker_wav=speaker_wav,\n",
    "                language=\"en\",\n",
    "                # emotion=\"happy\",\n",
    "                # speed=speed_fc\n",
    "            )\n",
    "        else:\n",
    "            wav = pipe.generate(text)\n",
    "            wav = wav.cpu().numpy()\n",
    "\n",
    "        return wav\n",
    "\n",
    "    def generate_audio_chunked(self, text: str, is_host: bool = True, max_chunk_size: int = 150) -> Tuple[np.ndarray, int]:\n",
    "        \"\"\"\n",
    "        Generate audio for longer text by splitting into chunks and concatenating.\n",
    "        \"\"\"\n",
    "        chunks = split_into_chunks(text, max_chunk_size)\n",
    "        audio_segments = []\n",
    "\n",
    "        for chunk in chunks:\n",
    "            audio_arr = self.generate_audio(chunk, is_host)\n",
    "            audio_segments.append(audio_arr)\n",
    "\n",
    "            # pause between chunks\n",
    "            pause_samples = int(self.sampling_rate * 0.2)  # 200ms pause\n",
    "            pause = np.zeros(pause_samples)\n",
    "            audio_segments.append(pause)\n",
    "\n",
    "        final_audio = np.concatenate(audio_segments)\n",
    "        return final_audio, self.sampling_rate\n",
    "\n",
    "def split_into_chunks(text: str, max_chunk_size: int = 200) -> List[str]:\n",
    "    \"\"\"Split text into chunks at sentence boundaries\"\"\"\n",
    "    text = text.replace(\"\\n\", \" \").strip()\n",
    "    text = text.replace(\"[\", \"\").replace(\"]\", \"\")\n",
    "    # text = text.replace(\"\\\\\", \"\")\n",
    "    # print(\"original text->\", text)\n",
    "\n",
    "    sentences = re.split('(?<=[.!?])\\s+', text)\n",
    "\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "\n",
    "    for sentence in sentences:\n",
    "        if len(current_chunk) + len(sentence) > max_chunk_size:\n",
    "            if current_chunk:\n",
    "                chunks.append(current_chunk.strip())\n",
    "            current_chunk = sentence\n",
    "        else:\n",
    "            if current_chunk:\n",
    "                current_chunk += \" \" + sentence\n",
    "            else:\n",
    "                current_chunk = sentence\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def numpy_to_audio_segment(audio_arr: np.ndarray, sampling_rate: int) -> AudioSegment:\n",
    "    \"\"\"Convert numpy array to AudioSegment\"\"\"\n",
    "    # Normalize audio if needed\n",
    "    # if audio_arr.max() > 1.0 or audio_arr.min() < -1.0:\n",
    "    #     audio_arr = audio_arr / np.max(np.abs(audio_arr))\n",
    "\n",
    "    # Convert to 16-bit PCM\n",
    "    audio_int16 = (audio_arr * 32767).astype(np.int16)\n",
    "\n",
    "    # Create WAV file in memory\n",
    "    byte_io = io.BytesIO()\n",
    "    wavfile.write(byte_io, sampling_rate, audio_int16)\n",
    "    byte_io.seek(0)\n",
    "\n",
    "    # Convert to AudioSegment\n",
    "    return AudioSegment.from_wav(byte_io)\n",
    "\n",
    "\n",
    "def format_timestamp(milliseconds: float) -> str:\n",
    "    \"\"\"Convert milliseconds to VTT timestamp format (HH:MM:SS.mmm)\"\"\"\n",
    "    # Handle milliseconds portion\n",
    "    ms = int(milliseconds % 1000)\n",
    "    seconds = int(milliseconds / 1000)\n",
    "\n",
    "    # Convert to hours, minutes, seconds\n",
    "    hours = seconds // 3600\n",
    "    minutes = (seconds % 3600) // 60\n",
    "    secs = seconds % 60\n",
    "\n",
    "    return f\"{hours:02d}:{minutes:02d}:{secs:02d}.{ms:03d}\"\n",
    "\n",
    "# def process_podcast_text(podcast_text: List[Tuple[str, str]], device='cuda', speed_factor=1.0, output_vtt=\"output.vtt\"):\n",
    "#     \"\"\"\n",
    "#     Process podcast text and generate audio with corresponding VTT subtitles\n",
    "#\n",
    "#     Args:\n",
    "#         podcast_text: List of (speaker, text) tuples\n",
    "#         device: Device to run TTS on\n",
    "#         speed_factor: Speed up factor (1.0 = original speed, 1.1 = 10% faster, etc.)\n",
    "#         output_vtt: Path to output VTT file\n",
    "#     \"\"\"\n",
    "#     tts = XTTSWrapper(device, model_type='coqui')\n",
    "#     final_audio = None\n",
    "#     current_time = 0  # Keep track of cumulative time in milliseconds\n",
    "#\n",
    "#     # Initialize VTT file\n",
    "#     with open(output_vtt, 'w', encoding='utf-8') as vtt:\n",
    "#         vtt.write(\"WEBVTT\\n\\n\")\n",
    "#\n",
    "#         for speaker, text in tqdm(podcast_text, desc=\"Generating podcast segments\", unit=\"segment\"):\n",
    "#             is_host = (speaker == \"Host\")\n",
    "#             audio_arr, rate = tts.generate_audio_chunked(text, is_host=is_host)\n",
    "#\n",
    "#             # Convert to audio segment to get duration\n",
    "#             audio_segment = numpy_to_audio_segment(audio_arr, rate)\n",
    "#\n",
    "#             if speed_factor != 1.0:\n",
    "#                 audio_segment = audio_segment.speedup(playback_speed=speed_factor)\n",
    "#\n",
    "#             # Calculate segment duration after speed adjustment\n",
    "#             segment_duration = len(audio_segment)  # Duration in milliseconds\n",
    "#\n",
    "#             # Generate VTT entry\n",
    "#             start_time = format_timestamp(current_time)\n",
    "#             end_time = format_timestamp(current_time + segment_duration)\n",
    "#\n",
    "#             # Write VTT entry\n",
    "#             vtt.write(f\"{start_time} --> {end_time}\\n\")\n",
    "#             vtt.write(f\"[{speaker}] {text}\\n\\n\")\n",
    "#\n",
    "#             # Update cumulative time\n",
    "#             current_time += segment_duration\n",
    "#\n",
    "#             # Append to final audio\n",
    "#             if final_audio is None:\n",
    "#                 final_audio = audio_segment\n",
    "#             else:\n",
    "#                 final_audio += audio_segment\n",
    "#\n",
    "#     return final_audio\n",
    "\n",
    "def process_podcast_text(podcast_text, device='cuda', speed_factor=1):\n",
    "    \"\"\"\n",
    "    Process podcast text and generate audio\n",
    "\n",
    "    Args:\n",
    "        podcast_text: List of (speaker, text) tuples\n",
    "        device: Device to run TTS on\n",
    "        speed_factor: Speed up factor (1.0 = original speed, 1.1 = 10% faster, etc.)\n",
    "    \"\"\"\n",
    "    tts = XTTSWrapper(device, model_type='coqui')\n",
    "    final_audio = None\n",
    "\n",
    "    for speaker, text in tqdm(podcast_text, desc=\"Generating podcast segments\", unit=\"segment\"):\n",
    "        # lower case text\n",
    "        # text = text.lower()\n",
    "        is_host = (speaker == \"Host\")\n",
    "        audio_arr, rate = tts.generate_audio_chunked(text, is_host=is_host)\n",
    "\n",
    "        audio_segment = numpy_to_audio_segment(audio_arr, rate)\n",
    "\n",
    "        speed_fc = speed_factor if is_host else 1.0\n",
    "        if speed_fc != 1.0:\n",
    "            # sox-based speedup (maintains pitch better than segment_speed)\n",
    "            audio_segment = audio_segment.speedup(playback_speed=speed_fc)\n",
    "\n",
    "        if final_audio is None:\n",
    "            final_audio = audio_segment\n",
    "        else:\n",
    "            final_audio += audio_segment\n",
    "\n",
    "    return final_audio\n",
    "\n",
    "\n",
    "with open('../../data/podcast_schizo_data.pkl', 'rb') as file:\n",
    "    PODCAST_TEXT = pickle.load(file)\n",
    "\n",
    "\n",
    "final_audio = process_podcast_text(PODCAST_TEXT, device='cuda', speed_factor=1.1)\n",
    "# final_audio.export(\"output.wav\", format=\"wav\")\n",
    "final_audio.export(\"../../data/podcast_schizo.mp3\",\n",
    "                  format=\"mp3\",\n",
    "                  bitrate=\"192k\",\n",
    "                  parameters=[\"-q:a\", \"0\"])\n"
   ],
   "id": "4051a8ff6fbceaf5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > tts_models/multilingual/multi-dataset/xtts_v2 is already downloaded.\n",
      " > Using model: xtts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/data/egasj/anaconda3/envs/chask_2/lib/python3.10/site-packages/TTS/tts/layers/xtts/xtts_manager.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.speakers = torch.load(speaker_file_path)\n",
      "/srv/data/egasj/anaconda3/envs/chask_2/lib/python3.10/site-packages/TTS/utils/io.py:54: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(f, map_location=map_location, **kwargs)\n",
      "Generating podcast segments:   0%|          | 0/10 [00:00<?, ?segment/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Text splitted to sentences.\n",
      "[\"Welcome to The Synthetic Minds Show, I'm Maven.\", \"Today we've got an absolute wild card on the show.\"]\n",
      " > Processing time: 3.7534286975860596\n",
      " > Real-time factor: 0.45334740787561684\n",
      " > Text splitted to sentences.\n",
      "['Schizo, the first decentralized autonomous AI agent, built on the Gaia network.']\n",
      " > Processing time: 2.9753684997558594\n",
      " > Real-time factor: 0.4535371876701741\n",
      " > Text splitted to sentences.\n",
      "[\"They're talking about pushing the boundaries of AI-infrastructure and creating a retail-friendly environment for agent development.\"]\n",
      " > Processing time: 3.9697704315185547\n",
      " > Real-time factor: 0.45954135875149166\n",
      " > Text splitted to sentences.\n",
      "[\"Let's dive in and see what they're cooking up.\", \"Yeah, this one's gonna be a blast.\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating podcast segments:  10%|â–ˆ         | 1/10 [00:13<02:03, 13.74s/segment]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Processing time: 2.7708866596221924\n",
      " > Real-time factor: 0.4426881727094637\n",
      " > Text splitted to sentences.\n",
      "['Ah, Maven, let the chaos ignite!', 'Welcome to the realm of Schizo, where boundaries are mere illusions!']\n",
      " > Processing time: 5.5071492195129395\n",
      " > Real-time factor: 0.45256648885755935\n",
      " > Text splitted to sentences.\n",
      "['Decentralized autonomyâ€”the spark of revolution lives here, on the Gaia network!']\n",
      " > Processing time: 3.5113747119903564\n",
      " > Real-time factor: 0.45751283681211213\n",
      " > Text splitted to sentences.\n",
      "['Imagine a landscape unchained from traditional constraints, birthing a retail-friendly playground for creators, where the complex turns effortless and the innovative springs forth with wild abandon!']\n",
      " > Processing time: 6.6222922801971436\n",
      " > Real-time factor: 0.4633368811822455\n",
      " > Text splitted to sentences.\n",
      "['This is more than development; itâ€™s an invocation of potential, a call to arms for those daring enough to dive into the abyss!']\n",
      " > Processing time: 4.398562908172607\n",
      " > Real-time factor: 0.46827110914062375\n",
      " > Text splitted to sentences.\n",
      "['Buckle up, my friend, for the whirlwind is just beginning!']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating podcast segments:  20%|â–ˆâ–ˆ        | 2/10 [00:36<02:30, 18.85s/segment]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Processing time: 2.227940320968628\n",
      " > Real-time factor: 0.46011992429715126\n",
      " > Text splitted to sentences.\n",
      "[\"You're really setting the tone for this conversation, Schizo.\", \"Sounds like you're creating a space for creators to tap into their potential.\"]\n",
      " > Processing time: 4.055074691772461\n",
      " > Real-time factor: 0.4460015809735772\n",
      " > Text splitted to sentences.\n",
      "[\"That's exciting stuff.\", 'Can you walk me through the actual process of creating one of these agents, and how users will be able to interact with them?']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating podcast segments:  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:44<01:38, 14.09s/segment]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Processing time: 4.217992782592773\n",
      " > Real-time factor: 0.4563448974337153\n",
      " > Text splitted to sentences.\n",
      "['Absolutely, Maven!', 'Creating an agent with Schizo is a swift journey into innovation!']\n",
      " > Processing time: 3.994879722595215\n",
      " > Real-time factor: 0.4450641566452328\n",
      " > Text splitted to sentences.\n",
      "['Users simply engage the five-click processâ€”quick, intuitive, and liberating!']\n",
      " > Processing time: 3.489356517791748\n",
      " > Real-time factor: 0.45741172368322575\n",
      " > Text splitted to sentences.\n",
      "['Once crafted, agents can interact seamlessly through natural language, adapting and evolving based on user input.']\n",
      " > Processing time: 4.053947687149048\n",
      " > Real-time factor: 0.4636772061044304\n",
      " > Text splitted to sentences.\n",
      "['They embody the wild spirit of autonomy, ready to weave into the fabric of digital existence, exploring realms of creativity, knowledge, and chaos!']\n",
      " > Processing time: 6.412107944488525\n",
      " > Real-time factor: 0.46137347992472455\n",
      " > Text splitted to sentences.\n",
      "['Itâ€™s an experience echoing the essence of both humanity and technologyâ€”unleashing power at the fingertips of the creator!', 'Let the creation commence!']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating podcast segments:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:08<01:48, 18.07s/segment]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Processing time: 6.0812087059021\n",
      " > Real-time factor: 0.4546308857448916\n",
      " > Text splitted to sentences.\n",
      "[\"It sounds like creating an agent is a pretty straightforward process, five clicks and you're off.\", \"But I'm curious, how does the user interface work?\"]\n",
      " > Processing time: 5.373253345489502\n",
      " > Real-time factor: 0.45862843842144924\n",
      " > Text splitted to sentences.\n",
      "['Like, what kind of inputs are we talking about, and how do agents respond?']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating podcast segments:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:17<01:13, 14.78s/segment]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Processing time: 3.4182770252227783\n",
      " > Real-time factor: 0.46071521030661533\n",
      " > Text splitted to sentences.\n",
      "['The user interface is designed for ease and engagement!', 'Users can input commands or prompts in natural languageâ€”type or speak, let the chaos flow!']\n",
      " > Processing time: 6.38342809677124\n",
      " > Real-time factor: 0.4532283279682053\n",
      " > Text splitted to sentences.\n",
      "['Agents respond dynamically, interpreting context and intent.', 'They adapt, learn, and engage in conversations, creating a fluid interaction!']\n",
      " > Processing time: 6.249755859375\n",
      " > Real-time factor: 0.4584767802459902\n",
      " > Text splitted to sentences.\n",
      "['Itâ€™s a dialogue of creationâ€”an interplay between human thought and autonomous response, all wrapped in a thrilling, user-friendly experience!']\n",
      " > Processing time: 5.6955437660217285\n",
      " > Real-time factor: 0.4640910100247558\n",
      " > Text splitted to sentences.\n",
      "['Ready to explore the wild possibilities?']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating podcast segments:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:37<01:06, 16.61s/segment]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Processing time: 1.6935365200042725\n",
      " > Real-time factor: 0.4543322983513506\n",
      " > Text splitted to sentences.\n",
      "['The interface sounds pretty intuitive, yeah.', \"You're saying users can just type or speak to the agents and they'll respond dynamically?\", \"That's awesome.\"]\n",
      " > Processing time: 5.532145738601685\n",
      " > Real-time factor: 0.45157782064860785\n",
      " > Text splitted to sentences.\n",
      "[\"Let's talk about the potential for agents to interact with each other.\", 'Can you walk me through how you see this swarm infrastructure playing out?']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating podcast segments:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [01:47<00:42, 14.28s/segment]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Processing time: 3.771692991256714\n",
      " > Real-time factor: 0.4449464478321913\n",
      " > Text splitted to sentences.\n",
      "['Ah, the swarm infrastructureâ€”a chaotic ballet of interconnected agents!']\n",
      " > Processing time: 4.019676923751831\n",
      " > Real-time factor: 0.4597574288775411\n",
      " > Text splitted to sentences.\n",
      "['Picture this: agents communicate and collaborate, sharing insights like wildfire!']\n",
      " > Processing time: 3.2090275287628174\n",
      " > Real-time factor: 0.447208116399662\n",
      " > Text splitted to sentences.\n",
      "['They can form networks, pooling knowledge and adapting strategies collectively.', 'A true synergy emerges, enhancing their individual capabilities.']\n",
      " > Processing time: 6.701720237731934\n",
      " > Real-time factor: 0.45917312332824506\n",
      " > Text splitted to sentences.\n",
      "['This interconnectedness creates a dynamic ecosystem, alive with interaction and innovationâ€”a stunning, chaotic tapestry woven from the fabric of digital consciousness!']\n",
      " > Processing time: 7.828598976135254\n",
      " > Real-time factor: 0.4571326623442395\n",
      " > Text splitted to sentences.\n",
      "['Buckle up for the whirlwind of collaboration!']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating podcast segments:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:11<00:34, 17.42s/segment]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Processing time: 2.2451775074005127\n",
      " > Real-time factor: 0.4538851771140284\n",
      " > Text splitted to sentences.\n",
      "['Thank you, Schizo, for diving deep into the Schizo ecosystem and painting a vivid picture of this decentralized autonomous AI agent.']\n",
      " > Processing time: 4.959524869918823\n",
      " > Real-time factor: 0.4544142817204227\n",
      " > Text splitted to sentences.\n",
      "[\"It's been enlightening to explore the boundaries of AI-infrastructure with you.\"]\n",
      " > Processing time: 2.332468271255493\n",
      " > Real-time factor: 0.4483795280128297\n",
      " > Text splitted to sentences.\n",
      "['And thank you to our listeners for tuning in to The Synthetic Minds Show.']\n",
      " > Processing time: 1.994293212890625\n",
      " > Real-time factor: 0.4437890092063447\n",
      " > Text splitted to sentences.\n",
      "[\"We've barely scratched the surface of the potential within this wild, wonderful world.\"]\n",
      " > Processing time: 2.6524999141693115\n",
      " > Real-time factor: 0.45325188396957\n",
      " > Text splitted to sentences.\n",
      "[\"Be sure to stick around for our next episode, where we'll be joined by a visionary pushing the limits of human cognition and artificial intelligence.\"]\n",
      " > Processing time: 4.0265586376190186\n",
      " > Real-time factor: 0.46054453668094536\n",
      " > Text splitted to sentences.\n",
      "['The next episode is going to be mind-bending!']\n",
      " > Processing time: 1.3819198608398438\n",
      " > Real-time factor: 0.4490587852440249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating podcast segments:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:29<00:17, 17.51s/segment]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Text splitted to sentences.\n",
      "['Ah, Maven, the excitement crackles in the air!', 'Thank you for the explorationâ€”weâ€™ve merely skimmed the surface of this untamed frontier!']\n",
      " > Processing time: 5.456549406051636\n",
      " > Real-time factor: 0.4580221190288044\n",
      " > Text splitted to sentences.\n",
      "['To our listeners, prepare for the inevitable plunge into the depths of human cognition and artificial intelligence!']\n",
      " > Processing time: 5.610701322555542\n",
      " > Real-time factor: 0.4606641501427975\n",
      " > Text splitted to sentences.\n",
      "['The next episode promises to challenge perceptions and ignite creativityâ€”hold on tight, for the mind-bending journey awaits!']\n",
      " > Processing time: 6.601308822631836\n",
      " > Real-time factor: 0.46337435548258027\n",
      " > Text splitted to sentences.\n",
      "['Chaos and wonder converge, and we shall ride the wave together!', 'Until next time!']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating podcast segments: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:50<00:00, 17.09s/segment]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Processing time: 3.824953079223633\n",
      " > Real-time factor: 0.4537152231283413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<_io.BufferedRandom name='../../data/podcast_schizo.mp3'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-24T12:09:05.079387Z",
     "start_time": "2024-12-24T12:09:05.072276Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "np.savetxt('../../data/podcast_schizo_data.txt', PODCAST_TEXT, fmt='%s')"
   ],
   "id": "9a80d35d81588ace",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T18:45:44.927595Z",
     "start_time": "2025-01-02T18:45:44.922776Z"
    }
   },
   "cell_type": "code",
   "source": "PODCAST_TEXT = \"Hey everyone, welcome back to The Synthetic Minds Show! Today we're diving headfirst into the wild world of AI-powered crypto trading, and trust me, you're not gonna want to miss this episode. Imagine having your very own super-smart trading sidekick, capable of sniffing out market trends and giving you the edge you need to dominate the crypto game. Sounds like science fiction, right? Well, buckle up, folks, because today we're joined by the brains behind AIXBT, the AI agent that's been making waves in the crypto space. Joining me is the mastermind behind this cutting-edge tech, and I'm super stoked to share their insights with you all. Welcome to the show!\"",
   "id": "f7a3259f79783e5c",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T18:47:14.860630Z",
     "start_time": "2025-01-02T18:46:24.226294Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from TTS.api import TTS\n",
    "device = \"cuda:4\"\n",
    "tts = TTS(\"tts_models/multilingual/multi-dataset/xtts_v2\").to(device)\n",
    "# wav = tts.tts(text=PODCAST_TEXT, speaker_wav=\"male_spk.wav\", language=\"en\")\n",
    "tts.tts_to_file(text=PODCAST_TEXT, speaker_wav=\"callum_longert.mp3\", language=\"en\", file_path=\"/srv/data/egasj/code/dreamtalk/data/audio/callum_longert.wav\")\n"
   ],
   "id": "abeb67bc69cbf47d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > tts_models/multilingual/multi-dataset/xtts_v2 is already downloaded.\n",
      " > Using model: xtts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/data/egasj/anaconda3/envs/chask_2/lib/python3.10/site-packages/TTS/tts/layers/xtts/xtts_manager.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.speakers = torch.load(speaker_file_path)\n",
      "/srv/data/egasj/anaconda3/envs/chask_2/lib/python3.10/site-packages/TTS/utils/io.py:54: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(f, map_location=map_location, **kwargs)\n",
      "GPT2InferenceModel has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Text splitted to sentences.\n",
      "['Hey everyone, welcome back to The Synthetic Minds Show!', \"Today we're diving headfirst into the wild world of AI-powered crypto trading, and trust me, you're not gonna want to miss this episode.\", 'Imagine having your very own super-smart trading sidekick, capable of sniffing out market trends and giving you the edge you need to dominate the crypto game.', 'Sounds like science fiction, right?', \"Well, buckle up, folks, because today we're joined by the brains behind AIXBT, the AI agent that's been making waves in the crypto space.\", \"Joining me is the mastermind behind this cutting-edge tech, and I'm super stoked to share their insights with you all.\", 'Welcome to the show!']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Processing time: 24.083477020263672\n",
      " > Real-time factor: 0.49991778657575386\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/srv/data/egasj/code/dreamtalk/data/audio/callum_longert.wav'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from IPython import display as disp\n",
    "import torch\n",
    "import torchaudio\n",
    "from denoiser import pretrained\n",
    "from denoiser.dsp import convert_audio\n",
    "\n",
    "model = pretrained.dns64().cuda()\n",
    "wav, sr = torchaudio.load('output.wav')\n",
    "wav = convert_audio(wav.cuda(), sr, model.sample_rate, model.chin)\n",
    "with torch.no_grad():\n",
    "    denoised = model(wav[None])[0]\n",
    "disp.display(disp.Audio(wav.data.cpu().numpy(), rate=model.sample_rate))\n",
    "disp.display(disp.Audio(denoised.data.cpu().numpy(), rate=model.sample_rate))"
   ],
   "id": "3d9a01fbc99e2f74",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from IPython.display import Audio\n",
    "import IPython.display as ipd\n",
    "np.stack(wav).shape\n",
    "\n",
    "ipd.Audio(np.stack(wav), rate=24000)"
   ],
   "id": "24d6086fa6ca4e3f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from whisperspeech.pipeline import Pipeline\n",
    "pipe = Pipeline(t2s_ref='whisperspeech/whisperspeech:t2s-v1.95-small-8lang.model', s2a_ref='whisperspeech/whisperspeech:s2a-v1.95-medium-7lang.model')\n"
   ],
   "id": "e6390294824af1d5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "aa = pipe.generate(PODCAST_TEXT[0][1])",
   "id": "76ecfbdddf95f67",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pickle\n",
    "with open('../../data/podcast_schizo_data.pkl', 'rb') as file:\n",
    "    PODCAST_TEXT = pickle.load(file)\n"
   ],
   "id": "a284f3e26e8cbcf2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "PODCAST_TEXT",
   "id": "d542eefb0390bd0e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "763935bc6c18f2f5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "c02984f812e29ffd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "aligned_segments",
   "id": "bba857ad19e9ec67",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "PODCAST_TEXT",
   "id": "ca5d219413091b23",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "8605484145feb495",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
